{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c04274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se sei in notebook:\n",
    "# !pip install -q \"transformers>=4.42.0\" \"datasets>=2.19.0\" \"accelerate>=0.31.0\" \"trl>=0.9.6\" \"peft>=0.11.1\" bitsandbytes sentencepiece evaluate\n",
    "#\n",
    "# Se sei in .py, installa da terminale prima di lanciare lo script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78a120f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10311bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('64bit', 'WindowsPE')\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.architecture())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411f0534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.6.0+cu124\n",
      "torch.version.cuda: 12.4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)  # None = build CPU-only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5994505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True  |  BF16: False  |  DTYPE: torch.float16\n"
     ]
    }
   ],
   "source": [
    "import os, random, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Rileva GPU e imposta dtype\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "BF16_OK = HAS_CUDA and torch.cuda.get_device_properties(0).total_memory > 20e9\n",
    "DTYPE = torch.bfloat16 if BF16_OK else torch.float16\n",
    "\n",
    "print(f\"CUDA: {HAS_CUDA}  |  BF16: {BF16_OK}  |  DTYPE: {DTYPE}\")\n",
    "\n",
    "# Cartella output per modelli e log\n",
    "OUT_DIR = Path(\"outputs/sft_lora_tinyllama\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Disattiva la parallelizzazione dei tokenizer (per log più puliti)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d0f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Scripts\\python.exe\n",
      "Torch: 2.5.1+cu121\n",
      "torch.version.cuda: 12.1\n",
      "is_available: True\n",
      "device_count: 1\n",
      "CUDA_VISIBLE_DEVICES: None\n",
      "GPU: NVIDIA RTX A3000 12GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import sys, torch, os\n",
    "print(\"Python:\", sys.executable)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"torch.version.cuda:\", torch.version.cuda)   # deve mostrare \"12.x\", NON None\n",
    "print(\"is_available:\", torch.cuda.is_available())\n",
    "print(\"device_count:\", torch.cuda.device_count())\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea6010",
   "metadata": {},
   "source": [
    "### Import, seed e configurazione base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9539710e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True  |  BF16: False  |  DTYPE: torch.float16\n"
     ]
    }
   ],
   "source": [
    "import os, random, torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Riproducibilità\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Rileva GPU e imposta dtype\n",
    "HAS_CUDA = torch.cuda.is_available()\n",
    "BF16_OK = HAS_CUDA and torch.cuda.get_device_properties(0).total_memory > 20e9\n",
    "DTYPE = torch.bfloat16 if BF16_OK else torch.float16\n",
    "\n",
    "print(f\"CUDA: {HAS_CUDA}  |  BF16: {BF16_OK}  |  DTYPE: {DTYPE}\")\n",
    "\n",
    "# Cartella output per modelli e log\n",
    "OUT_DIR = Path(\"outputs/sft_lora_tinyllama\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Disattiva la parallelizzazione dei tokenizer (per log più puliti)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1e4d5",
   "metadata": {},
   "source": [
    "### Scelta modello e tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81209fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\laudi\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Modello piccolo \"chatty\" compatibile con LoRA/QLoRA\n",
    "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Carica tokenizer (fast) e definisci il pad_token se assente\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, padding_side=\"right\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168980f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "082f80c2",
   "metadata": {},
   "source": [
    "### Caricamento dataset (Databricks Dolly 15k) e formattazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76531620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\laudi\\.cache\\huggingface\\hub\\datasets--databricks--databricks-dolly-15k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Generating train split: 100%|██████████| 15011/15011 [00:02<00:00, 6570.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'context', 'response', 'category'],\n",
      "        num_rows: 15011\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15011/15011 [00:02<00:00, 5066.78 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 15011\n",
      "    })\n",
      "})\n",
      "Esempio: <s>[SYSTEM]\n",
      "Sei un assistente utile, educato e competente.\n",
      "[/SYSTEM]\n",
      "[USER]\n",
      "When did Virgin Australia start operating?\n",
      "Contesto: Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It sudde ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Carica l'intero dataset (train/test/val non sempre presenti: Dolly ha 'train' e a volte 'test')\n",
    "# https://huggingface.co/datasets/databricks/databricks-dolly-15k\n",
    "raw_ds = load_dataset(\"databricks/databricks-dolly-15k\")\n",
    "print(raw_ds)\n",
    "\n",
    "# Dolly ha campi tipici: instruction, context, response\n",
    "# Creiamo un formato testuale unico per l'SFT con un semplice \"template\" stile chat.\n",
    "SYSTEM = \"Sei un assistente utile, educato e competente.\"\n",
    "\n",
    "def format_record(ex):\n",
    "    instr = ex.get(\"instruction\", \"\") or \"\"\n",
    "    ctx   = ex.get(\"context\", \"\") or \"\"\n",
    "    resp  = ex.get(\"response\", \"\") or \"\"\n",
    "    if ctx.strip():\n",
    "        prompt = (\n",
    "            f\"<s>[SYSTEM]\\n{SYSTEM}\\n[/SYSTEM]\\n\"\n",
    "            f\"[USER]\\n{instr}\\nContesto: {ctx}\\n[/USER]\\n[ASSISTANT]\\n\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            f\"<s>[SYSTEM]\\n{SYSTEM}\\n[/SYSTEM]\\n\"\n",
    "            f\"[USER]\\n{instr}\\n[/USER]\\n[ASSISTANT]\\n\"\n",
    "        )\n",
    "    text = prompt + resp  # per causal LM mettiamo prompt + risposta come unico testo\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Applica il formatter e rimuovi le colonne originali\n",
    "ds = raw_ds.map(format_record, remove_columns=raw_ds[\"train\"].column_names)\n",
    "print(ds)\n",
    "print(\"Esempio:\", ds[\"train\"][0][\"text\"][:400], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646b2eb",
   "metadata": {},
   "source": [
    "### (Opzionale) QLoRA 4-bit: quantizzazione per ridurre VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2192a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Config per caricare il modello in 4-bit (QLoRA). Richiede GPU + bitsandbytes.\n",
    "# Se NON hai GPU, SALTA questa sezione e carica il modello \"normale\" in STEP 5.\n",
    "bnb_config = None\n",
    "if HAS_CUDA:\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                 # attiva quantizzazione 4-bit\n",
    "        bnb_4bit_quant_type=\"nf4\",         # quantizzazione \"normal float 4\"\n",
    "        bnb_4bit_use_double_quant=True,    # double quant per meno memoria\n",
    "        bnb_4bit_compute_dtype=DTYPE       # compute in bf16/fp16\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46310e",
   "metadata": {},
   "source": [
    "### Carica il modello e “avvolgilo” con LoRA (PEFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85a335c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametri trainabili (LoRA) vs totali:\n",
      "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Carica il modello base: con quantizzazione se disponibile, altrimenti in fp16/bf16 quindi usiamo QLoRa\n",
    "if bnb_config is not None:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    # CPU o GPU senza quantizzazione: attenzione ai consumi\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\" if HAS_CUDA else None\n",
    "    )\n",
    "\n",
    "# Configurazione LoRA:\n",
    "# r: rank delle matrici low-rank; lora_alpha: scaling; lora_dropout: regolarizzazione\n",
    "# target_modules: i sotto-layer su cui applicare LoRA (tipico per famiglie LLaMA)\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]  # puoi includere anche gate/up/down_proj\n",
    ")\n",
    "\n",
    "# Avvolgi il modello con PEFT-LoRA\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "# Abilita gradient checkpointing per ridurre memoria (leggero overhead computazionale)\n",
    "model.gradient_checkpointing_enable()\n",
    "print(\"Parametri trainabili (LoRA) vs totali:\")\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f0cd0",
   "metadata": {},
   "source": [
    "### Trainer SFT (TRL) con collator e argomenti di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.21,>=2.20 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tf-keras) (2.20.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.3.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\laudi\\onedrive\\documenti\\repo\\deeplearning_machinelearning\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.8/1.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.7 MB/s  0:00:01\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.20.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9457ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Adding EOS to train dataset: 100%|██████████| 5000/5000 [00:00<00:00, 21527.09 examples/s]\n",
      "Tokenizing train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2294 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing train dataset: 100%|██████████| 5000/5000 [00:03<00:00, 1285.89 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 5000/5000 [00:00<00:00, 263107.63 examples/s]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
      "c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 1:17:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.220200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.787100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.767200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.649600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.584700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.566700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.587600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.391200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.583800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.479500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.483400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.576600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.568100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.572200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.391600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.488400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.546900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.511300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.523300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.497900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.459900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.443800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.450100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.493100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.348800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.527500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.470500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.589600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.513700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.430600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.520900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.515100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.560900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.389900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>1.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.386600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>1.567400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.449100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>1.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>1.427200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.490900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>1.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>1.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.514600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>1.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.350600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.545900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.495400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.419800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>1.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>1.517100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>1.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.537100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.383400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.505400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>1.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.433700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>1.426900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.511500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.391900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>1.492500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.474200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.370700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>1.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>1.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.297700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.368000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.380900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.451100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.468800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.434900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.447700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.502700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.456300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.401700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.438100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.483900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.465100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.403200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.515600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>1.489400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>1.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>1.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.442900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>1.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.377900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>1.483700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.444200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>1.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>1.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.508300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.370600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.351700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.507100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>1.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.503000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>1.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.392200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>1.357700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\laudi\\OneDrive\\Documenti\\REPO\\DeepLearning_MachineLearning\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=1.4763242940266927, metrics={'train_runtime': 4684.8568, 'train_samples_per_second': 3.202, 'train_steps_per_second': 0.4, 'total_flos': 5.486324167272038e+16, 'train_loss': 1.4763242940266927, 'entropy': 1.3060927152633668, 'num_tokens': 3537546.0, 'mean_token_accuracy': 0.7046420693397522, 'epoch': 3.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Crea config SFT senza peft_config dentro\n",
    "sft_cfg = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=1024,\n",
    "    assistant_only_loss=False,\n",
    "    completion_only_loss=True,\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "# Crea config LoRA (PEFT)\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_cfg,\n",
    "    train_dataset=ds[\"train\"].select(range(5000)),\n",
    "    peft_config=lora_cfg,\n",
    "    data_collator=collator,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33282c5f",
   "metadata": {},
   "source": [
    "### Salvataggio adapter LoRA e tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "618ae64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter LoRA salvato#  in: outputs\\sft_lora_tinyllama\\lora_adapter\n"
     ]
    }
   ],
   "source": [
    "ADAPTER_DIR = OUT_DIR / \"lora_adapter\"\n",
    "ADAPTER_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Salva SOLO i pesi LoRA (pochi MB) + tokenizer\n",
    "trainer.model.save_pretrained(str(ADAPTER_DIR))\n",
    "tokenizer.save_pretrained(str(ADAPTER_DIR))\n",
    "print(\"Adapter LoRA salvato#  in:\", ADAPTER_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302eccb",
   "metadata": {},
   "source": [
    "### Inferenzia con l’adapter (senza fare merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b28ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[SYSTEM]\n",
      "Sei un assistente fiscale conciso.\n",
      "[/SYSTEM]\n",
      "[USER]\n",
      "Spiega l'IVA in Italia in 3 punti chiari.\n",
      "[/USER]\n",
      "[ASSISTANT]\n",
      "In Italia, l'IVA è una forma di cassa fiscale che è stata istituita nel 1924. È una cassa fiscale che è stata istituita con l'intento di migliorare la regolamentazione dell'economia del paese. L'IVA è la cassa fiscale che viene utilizzata per i salari e gli imposti a cassa fiscale. L'IVA è una cassa fiscale che è stata istituita con l'intento di migliorare la regolamentazione dell'economia del paese.\n",
      "\n",
      "In Italia, l'IVA è una forma di cassa fiscale che è stata istituita nel 1924. L'IVA è una cassa fiscale che è stata istituita con l'intento di migliorare la regolamentazione dell'\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import pipeline, AutoModelForCausalLM\n",
    "\n",
    "# Ricarica il modello base (quantizzato se possibile) e attacca l'adapter LoRA\n",
    "if bnb_config is not None:\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=DTYPE,\n",
    "        device_map=\"auto\" if HAS_CUDA else None\n",
    "    )\n",
    "\n",
    "ft = PeftModel.from_pretrained(base, str(ADAPTER_DIR))\n",
    "\n",
    "# Pipeline di generazione\n",
    "gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=ft,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "prompt = (\n",
    "    \"<s>[SYSTEM]\\nSei un assistente fiscale conciso.\\n[/SYSTEM]\\n\"\n",
    "    \"[USER]\\nSpiega l'IVA in Italia in 3 punti chiari.\\n[/USER]\\n[ASSISTANT]\\n\"\n",
    ")\n",
    "out = gen(prompt, max_new_tokens=200, do_sample=True, temperature=0.7, top_p=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "daed42f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"<s>[SYSTEM]\\nSei un assistente fiscale conciso.\\n[/SYSTEM]\\n[USER]\\nSpiega l'IVA in Italia in 3 punti chiari.\\n[/USER]\\n[ASSISTANT]\\nIn Italia, l'IVA è una forma di cassa fiscale che è stata istituita nel 1924. È una cassa fiscale che è stata istituita con l'intento di migliorare la regolamentazione dell'economia del paese. L'IVA è la cassa fiscale che viene utilizzata per i salari e gli imposti a cassa fiscale. L'IVA è una cassa fiscale che è stata istituita con l'intento di migliorare la regolamentazione dell'economia del paese.\\n\\nIn Italia, l'IVA è una forma di cassa fiscale che è stata istituita nel 1924. L'IVA è una cassa fiscale che è stata istituita con l'intento di migliorare la regolamentazione dell'\"}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bb5324",
   "metadata": {},
   "source": [
    "### (Opzionale) Merge LoRA nel backbone per esportare un singolo checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf9ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENZIONE: il merge richiede più memoria (temporaneamente).\n",
    "merged = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" if HAS_CUDA else None\n",
    ")\n",
    "merged = PeftModel.from_pretrained(merged, str(ADAPTER_DIR))\n",
    "merged = merged.merge_and_unload()  # fonde gli adapter nei pesi del modello\n",
    "\n",
    "MERGED_DIR = OUT_DIR / \"merged_full\"\n",
    "MERGED_DIR.mkdir(exist_ok=True, parents=True)\n",
    "merged.save_pretrained(str(MERGED_DIR))\n",
    "tokenizer.save_pretrained(str(MERGED_DIR))\n",
    "print(\"Modello fuso salvato in:\", MERGED_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9b89f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
